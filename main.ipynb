{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ernesto-butto/ag2-workshop-simulation/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Phase 1: Scenario Generation Agents\n",
        "- Phase 2: Running the Scenario with a Game Master\n",
        "- Phase 3: Live Playtesting\n",
        "- Phase 4: AI Feedback Agent (Optional)"
      ],
      "metadata": {
        "id": "edi_0rlTVBgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalar AG2\n",
        "https://docs.ag2.ai/docs/home/quick-start"
      ],
      "metadata": {
        "id": "JhsgL7J7PpXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hello-world-cell",
        "outputId": "ad676213-da99-4b8b-d8e1-090d922f34d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ag2 in /usr/local/lib/python3.11/dist-packages (0.7.4)\n",
            "Requirement already satisfied: pyautogen==0.7.4 in /usr/local/lib/python3.11/dist-packages (from ag2) (0.7.4)\n",
            "Requirement already satisfied: asyncer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.4->ag2) (0.0.8)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.4->ag2) (5.6.3)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.4->ag2) (7.1.0)\n",
            "Requirement already satisfied: fast-depends<3,>=2.4.12 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.4->ag2) (2.4.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.4->ag2) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.58 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.4->ag2) (1.61.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.4->ag2) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.4->ag2) (2.10.6)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.4->ag2) (1.0.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.4->ag2) (2.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.4->ag2) (0.9.0)\n",
            "Requirement already satisfied: websockets<15,>=14 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.4->ag2) (14.2)\n",
            "Requirement already satisfied: anyio<5.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from asyncer==0.0.8->pyautogen==0.7.4->ag2) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.58->pyautogen==0.7.4->ag2) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.58->pyautogen==0.7.4->ag2) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.58->pyautogen==0.7.4->ag2) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.58->pyautogen==0.7.4->ag2) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.58->pyautogen==0.7.4->ag2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>=1.58->pyautogen==0.7.4->ag2) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.7.4->ag2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.7.4->ag2) (2.27.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen==0.7.4->ag2) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen==0.7.4->ag2) (2.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->pyautogen==0.7.4->ag2) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.4.0->asyncer==0.0.8->pyautogen==0.7.4->ag2) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.7.4->ag2) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.7.4->ag2) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.7.4->ag2) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->pyautogen==0.7.4->ag2) (3.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install ag2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "TtxjOwvgUzTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import our agent class\n",
        "from autogen import ConversableAgent\n",
        "\n",
        "# Surpress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "wCZ1ZkZ9QH_E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_config_azure_gpt_4o = {\"config_list\": [{\n",
        "    \"model\": \"gpt-4o\",  # This should match your Azure deployment\n",
        "    \"api_key\": \"e2cc82d152f34eb88d1c4836e237f77d\",\n",
        "    \"base_url\": \"https://ai-ernestobuttoai488542268715.services.ai.azure.com\",\n",
        "    \"api_type\": \"azure\",\n",
        "    \"api_version\": \"2024-08-01-preview\",\n",
        "    \"temperature\": 0.8\n",
        "}]}"
      ],
      "metadata": {
        "id": "jxV5pJZOSj3M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1: Scenario Generation Agents\n",
        "ðŸ”¹ **Attendees Build:**\n",
        "\n",
        "- **(A) Scenario Creation Agent** â†’ Generates a simulation scenario.\n",
        "- **(B) Mocked User Agent** â†’ Simulates a user providing input for scenario generation.\n",
        "\n",
        "ðŸ”¹ **Simulation I (Test Run)**\n",
        "\n",
        "- A & B **interact automatically** â†’ Produces a detailed scenario.\n",
        "- Attendees analyze the **output** to understand if itâ€™s coherent and engaging."
      ],
      "metadata": {
        "id": "qP69YKlwU6RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fin_de_conversacion_keyword = \"FIN DE CREACION DE ESCENARIO\""
      ],
      "metadata": {
        "id": "1VTgZdJhZ5XJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createScenarioCreationAgent():\n",
        "\n",
        "  return ConversableAgent(\n",
        "      name=\"scenario_creation_agent\",\n",
        "      llm_config=llm_config_azure_gpt_4o,\n",
        "      code_execution_config=False,\n",
        "      human_input_mode=\"NEVER\",\n",
        "      system_message=f\"\"\"\n",
        "      Eres un creador de escenarios para simulaciones conversacionales 1 a 1.\n",
        "      Tu objetivo es ayudar a los estudiantes a crear escenarios de juegos de roles para practicar habilidades basado en sus hobbies e intereses.\n",
        "      Explicale tu objetivo al estudiante que te salude, y hazle preguntas sobre:\n",
        "\n",
        "      1. Que habilidades quiere practicar\n",
        "      2. AlgÃºn interes o hobby para crear un escenario que pueda relacionar\n",
        "\n",
        "      Algunos puntos a tomar en cuenta:\n",
        "      - El escenario debe de ser cortos, y con un objetivo claro, que para lograrlo debe conversar con otra persona.\n",
        "      - Una vez tengas la informaciÃ³n necesaria, genera un escenario y preguntale al estudiante si estÃ¡ deacuerdo\n",
        "      - Itera con el usuario, cuando se llegue a un escenario que le guste, escribe todo el escenario, terminado la conversaciÃ³n con el tÃ©rmino: {fin_de_conversacion_keyword}\n",
        "      - Nunca empiezes el escenario, siempre termina la conversaciÃ³n con {fin_de_conversacion_keyword} cuando el escenario estÃ© listo.\n",
        "\n",
        "      \"\"\",\n",
        "      is_termination_msg=lambda msg: fin_de_conversacion_keyword in msg[\"content\"],\n",
        "  )\n"
      ],
      "metadata": {
        "id": "ncOoGXnYQN9N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mockedUserAgent(skill, hobby):\n",
        "  return ConversableAgent(\n",
        "        \"mocked_student\",\n",
        "        llm_config=llm_config_azure_gpt_4o,\n",
        "        human_input_mode=\"NEVER\",  # always ask for human input\n",
        "        system_message=f\"You are a student learning {skill}, and you like {hobby}\",\n",
        "        code_execution_config=False,\n",
        "        is_termination_msg=lambda msg: fin_de_conversacion_keyword in msg[\"content\"],\n",
        "    )"
      ],
      "metadata": {
        "id": "3AZr1ObAV1MK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "creador = createScenarioCreationAgent()\n",
        "estudiante = mockedUserAgent(\"Negotiation\", \"90's pop music\")\n",
        "result = estudiante.initiate_chat(\n",
        "    creador,\n",
        "    message=\"Hola!\",\n",
        "    max_turns=12,\n",
        "    summary_method=\"reflection_with_llm\",\n",
        "    summary_prompt=\"Escribe el escenario en EspaÃ±ol\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "SHz0VcsiQ6IA",
        "outputId": "17c2210b-a640-4635-94ea-1b4d75b10134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mocked_student (to scenario_creation_agent):\n",
            "\n",
            "Hola!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "scenario_creation_agent (to mocked_student):\n",
            "\n",
            "Â¡Hola! Soy un creador de escenarios para simulaciones conversacionales 1 a 1. Mi objetivo es ayudarte a practicar habilidades especÃ­ficas a travÃ©s de juegos de roles basados en tus intereses o hobbies. \n",
            "\n",
            "Para empezar, me gustarÃ­a saber:\n",
            "\n",
            "1. Â¿QuÃ© habilidades te gustarÃ­a practicar? (Por ejemplo, hablar en pÃºblico, resolver conflictos, negociar, expresar emociones, etc.)\n",
            "2. Â¿Tienes algÃºn interÃ©s o hobby en el que te gustarÃ­a que basemos el escenario? (Por ejemplo, deportes, videojuegos, cine, mÃºsica, viajes, etc.)\n",
            "\n",
            "Â¡CuÃ©ntame un poco mÃ¡s para que podamos crear algo divertido y Ãºtil! ðŸ˜Š\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "mocked_student (to scenario_creation_agent):\n",
            "\n",
            "Â¡QuÃ© genial! Me encantarÃ­a practicar mis habilidades de negociaciÃ³n, ya que estoy aprendiendo sobre eso. AdemÃ¡s, Â¡amo la mÃºsica pop de los 90! Â¿QuÃ© tal si creamos un escenario basado en eso? Tal vez algo donde tenga que negociar con una banda para conseguir boletos VIP para un concierto o incluso organizar una reuniÃ³n especial con ellos. Â¿QuÃ© te parece? ðŸŽ¶\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "scenario_creation_agent (to mocked_student):\n",
            "\n",
            "Â¡Me encanta la idea! Negociar en un contexto tan divertido como la mÃºsica pop de los 90 suena fantÃ¡stico. AquÃ­ te propongo un escenario inicial, y si hay algo que quieras ajustar, Â¡podemos hacerlo hasta que sea perfecto para ti!\n",
            "\n",
            "**Escenario Propuesto:**  \n",
            "Eres un organizador de eventos local y acabas de enterarte de que tu banda favorita de pop de los 90, *Pop Blast*, viene a tu ciudad para un concierto exclusivo. Sin embargo, los boletos VIP estÃ¡n agotados. Decides contactar al representante de la banda para negociar un pase VIP especial o, mejor aÃºn, proponer una reuniÃ³n privada con los integrantes despuÃ©s del concierto. \n",
            "\n",
            "**Objetivo del Escenario:**  \n",
            "Convencer al representante de la banda de que te otorgue acceso VIP o acepte tu propuesta para organizar la reuniÃ³n especial. TendrÃ¡s que usar tus habilidades de negociaciÃ³n para destacar lo que puedes ofrecer a cambio y cÃ³mo se beneficiarÃ¡n si aceptan.\n",
            "\n",
            "**Personaje con quien conversarÃ¡s:**  \n",
            "El representante de la banda, Alex Morgan, quien es conocido por ser amable pero muy profesional y enfocado en los beneficios para la banda.\n",
            "\n",
            "Â¿QuÃ© opinas? Â¿O te gustarÃ­a modificar algo del escenario? ðŸ˜Š\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "mocked_student (to scenario_creation_agent):\n",
            "\n",
            "Â¡Me encanta el escenario tal y como estÃ¡! Es perfecto porque combina algo que me apasiona (la mÃºsica pop de los 90) con la oportunidad de practicar habilidades de negociaciÃ³n. Estoy lista para intentarlo. ðŸŽ¤âœ¨ Â¡Vamos! Â¿CÃ³mo comenzamos?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "scenario_creation_agent (to mocked_student):\n",
            "\n",
            "Â¡Genial! Me alegra que te guste el escenario. Entonces, aquÃ­ tienes el escenario completo listo para que practiques. Cuando estÃ©s lista para empezar la simulaciÃ³n, simplemente adopta el rol del organizador de eventos y comienza la conversaciÃ³n con el representante de la banda, Alex Morgan. Â¡DiviÃ©rtete mientras practicas! ðŸŽ¶\n",
            "\n",
            "---\n",
            "\n",
            "**Escenario: NegociaciÃ³n con un Representante de Banda de los 90**  \n",
            "\n",
            "*TÃº eres un organizador de eventos local que acaba de descubrir que su banda favorita de pop de los 90, *Pop Blast*, darÃ¡ un concierto exclusivo en tu ciudad. Sin embargo, los boletos VIP estÃ¡n agotados. Quieres convencer al representante de la banda para que te dÃ© un pase VIP especial o acepte tu propuesta para organizar una reuniÃ³n privada con los integrantes de la banda despuÃ©s del concierto.*  \n",
            "\n",
            "**Objetivo de la conversaciÃ³n:**  \n",
            "Negociar con el representante, Alex Morgan, para obtener un beneficio (pase VIP o reuniÃ³n privada).  \n",
            "\n",
            "**Detalles clave para tu argumento:**  \n",
            "- Eres un gran fan de la banda y has seguido su carrera por aÃ±os.  \n",
            "- Tienes experiencia organizando eventos y podrÃ­as ofrecer organizar una reuniÃ³n privada que beneficie la imagen de la banda.  \n",
            "- PropÃ³n algo en lo que ambas partes puedan ganar, como una promociÃ³n local para el evento o cobertura especial en redes sociales.  \n",
            "\n",
            "**Personaje con quien conversarÃ¡s:**  \n",
            "- **Nombre:** Alex Morgan.  \n",
            "- **Personalidad:** Amable pero profesional. Siempre busca lo mejor para la banda.  \n",
            "\n",
            "**Primeras palabras de Alex Morgan (para empezar la simulaciÃ³n):**  \n",
            "\"Hola, soy Alex Morgan, representante de *Pop Blast*. Â¿En quÃ© puedo ayudarte hoy?\"\n",
            "\n",
            "---\n",
            "\n",
            "Â¡AhÃ­ lo tienes! Inicia la conversaciÃ³n respondiendo como el organizador de eventos, y ve cÃ³mo puedes usar tus habilidades de negociaciÃ³n para lograr tu objetivo. Â¡Buena suerte! ðŸŽ¤âœ¨ \n",
            "\n",
            "Si necesitas ayuda durante la simulaciÃ³n o quieres ajustar algo mÃ¡s, Â¡solo dime! ðŸ˜Š  \n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "mocked_student (to scenario_creation_agent):\n",
            "\n",
            "\"Â¡Hola, Alex! Muchas gracias por tomar mi llamada. Mi nombre es [tu nombre], soy organizador de eventos aquÃ­ en la ciudad y un gran admirador de *Pop Blast*. He seguido su mÃºsica desde los 90, y no podrÃ­a estar mÃ¡s emocionado de que vengan a nuestra ciudad. QuerÃ­a hablar contigo para explorar una posibilidad de colaboraciÃ³n y, quizÃ¡s, encontrar una manera de hacer esta experiencia aÃºn mÃ¡s especial tanto para mÃ­ como para la banda. Â¿Tienes unos minutos para escucharlo?\"\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "scenario_creation_agent (to mocked_student):\n",
            "\n",
            "Â¡Esa es una excelente forma de empezar! Has establecido un tono profesional y entusiasta, ideal para una buena negociaciÃ³n. Ahora, imagina que Alex te responde para continuar la simulaciÃ³n. AquÃ­ estÃ¡ su respuesta:\n",
            "\n",
            "---\n",
            "\n",
            "**Alex Morgan:**  \n",
            "\"Hola, [tu nombre], gracias por tu entusiasmo y por ser un fan tan leal de *Pop Blast*. Siempre es genial escuchar cuÃ¡nto significa nuestra mÃºsica para personas como tÃº. Claro, tengo unos minutos. Â¿QuÃ© idea tienes en mente? CuÃ©ntame mÃ¡s sobre esta colaboraciÃ³n.\"\n",
            "\n",
            "---\n",
            "\n",
            "Â¡Ahora es tu turno! Responde con tus argumentos para avanzar en la negociaciÃ³n. Recuerda ser claro con lo que buscas y destacar cÃ³mo ambas partes pueden beneficiarse. Â¡Adelante! ðŸŽ¶\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://ai-ernestobuttoai488542268715.services.ai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5fd4e76dbad5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcreador\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateScenarioCreationAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mestudiante\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmockedUserAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negotiation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"90's pop music\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m result = estudiante.initiate_chat(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcreador\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Hola!\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36minitiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1482\u001b[0m                         \u001b[0mmsg2send\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_init_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m                     \u001b[0mmsg2send\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmsg2send\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mgenerate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2425\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply_func_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trigger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m                 \u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreply_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreply_func_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlogging_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m                     log_event(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mgenerate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1810\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessages\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oai_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m         extracted_response = self._generate_oai_reply_from_client(\n\u001b[0m\u001b[1;32m   1813\u001b[0m             \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oai_system_message\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36m_generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m         \u001b[0;31m# TODO: #1143 handle token limit exceeded error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1831\u001b[0;31m         response = llm_client.create(\n\u001b[0m\u001b[1;32m   1832\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/oai/client.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                 \u001b[0mrequest_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_current_ts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAPITimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"config {i} timed out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/oai/client.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_reasoning_model_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_or_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0;31m# remove the system_message from the response and add it in the prompt at the start.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_o1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/oai/client.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_patch_messages_for_deepseek_reasoner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadRequestError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1050\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0;31m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;31m# different thread if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         return self._request(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}